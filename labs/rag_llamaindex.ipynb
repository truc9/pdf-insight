{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface llama-index-graph-stores-neo4j graspologic numpy==1.24.4 scipy==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement textwrap (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for textwrap\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU llama-index-vector-stores-chroma\n",
    "%pip install -qU llama-index-embeddings-huggingface\n",
    "%pip install -qU llama-index-llms-ollama\n",
    "%pip install -qU textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, ChatPromptTemplate, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the programming skills listed for Truc Nguyen:\n",
      "\n",
      "1. Programming languages:\n",
      "\t* C#\n",
      "\t* JavaScript (ES)/TypeScript\n",
      "\t* Dart\n",
      "2. Backend technologies:\n",
      "\t* .NET Core\n",
      "\t* ASP.NET Core\n",
      "\t* ASP.NET WebAPI\n",
      "3. Testing frameworks and libraries:\n",
      "\t* NUnit\n",
      "\t* MSTest with Moq\n",
      "4. Frontend frameworks:\n",
      "\t* Angular\n",
      "\t* AngularJS\n",
      "\t* React\n",
      "5. Other programming skills:\n",
      "\t* Unit testing\n",
      "\t* TDD (Test-Driven Development)\n",
      "6. Databases:\n",
      "\t* SQL Server\n",
      "\t* SQLite\n",
      "\n",
      "Let me know if you need any further assistance!"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_overlap = 5\n",
    "Settings.chunk_size = 512\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./data/chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"rag_chatbot\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "index.storage_context.persist(persist_dir=\"./data/vectorstore\")\n",
    "\n",
    "# query_engine = index.as_query_engine(similarity_top_k=1)\n",
    "# response = query_engine.query(\"Tell me about Truc Nguyen?\")\n",
    "# display(Markdown(response.response))\n",
    "# response.print_response_stream()\n",
    "\n",
    "query = \"List all programming skills of Truc Nguyen?\"\n",
    "chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode=\"context\")\n",
    "response = chat_engine.stream_chat(query)\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "# display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_templates = [\n",
    "    ChatMessage(content=\"You are an expert system\", role=MessageRole.SYSTEM),\n",
    "    ChatMessage(\n",
    "        content=\"Generate a short story about {topic}\",\n",
    "        role=MessageRole.USER,\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_template = ChatPromptTemplate(message_templates=message_templates)\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    topic=\"Writing good CV to pass CV screening at FAANG companies\"\n",
    ")\n",
    "\n",
    "stream_resp = llm.stream_chat(messages)\n",
    "for r in stream_resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
